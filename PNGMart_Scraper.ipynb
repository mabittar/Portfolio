{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PNGMart-Scraper.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMMvoQtbgAVklLdrIoXIH0h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mabittar/Portfolio/blob/master/PNGMart_Scraper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZqXR7eBdzrs"
      },
      "source": [
        "# Web Scraper \r\n",
        "\r\n",
        "Nesse web scraper  veremos como criar um algorítmo para baixar todas as imagens, de forma classificada do site: pngmart.com utilizando o [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/).\r\n",
        "\r\n",
        "Esse é apenas um exemplo de aplicação de um Crawler, que pode ser utilizado para registrar preços de um produto, oportunidades de imóveis para venda ou locação, disponibilidade de servidores e assim por diante.\r\n",
        "\r\n",
        "Web Crawler, bot ou web spider é um algoritmo utilizado para encontrar, ler e gravar informações de páginas da internet. É como um robô que varre o caminho indicado e captura as informações que encontra pela frente.\r\n",
        "\r\n",
        "Um dos maiores exemplos de um web crawler é o próprio google. Antes do site estar disponível para pesquisa, um robô lê o web site e cataloga as informações de forma a serem recuperadas numa busca futura.\r\n",
        "\r\n",
        "\r\n",
        "Para esse web scrapper dividi o script em funções específicas a fim de facilitar qualquer possível manutenção.\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRm5t1zmb3Lp"
      },
      "source": [
        "# importando as bibliotecas necessárias\r\n",
        "import requests\r\n",
        "from bs4 import BeautifulSoup"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJfvySeLrFpv"
      },
      "source": [
        "## O Alvo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hea5rvq4c1X8"
      },
      "source": [
        "# link para o site map\r\n",
        "link = 'http://www.pngmart.com/sitemapindex.xml/'"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6jOeSTgrIqo"
      },
      "source": [
        "O site está dividido em diversas subpáginas, uma alternativa para buscarmos em quais páginas as imagens estão listadas foi utilizar o site map."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0Yoi_3gdEL3"
      },
      "source": [
        "# função para extrair as subpáginas que listam as imagens\r\n",
        "def get_url(link):\r\n",
        "  \"\"\"\r\n",
        "  essa função mapeia o sitemap e retorna as subpáginas onde as imagens estão listadas\r\n",
        "  entrada: link para o sitemap\r\n",
        "  saída: lista de páginas onde as imagens são listadas\r\n",
        "  \"\"\"\r\n",
        "    response = requests.get(link)\r\n",
        "    xml = response.text\r\n",
        "    soup = BeautifulSoup(xml, 'xml')\r\n",
        "\r\n",
        "    site_maps = []\r\n",
        "\r\n",
        "    for loc in soup.find_all('loc'):\r\n",
        "      url = loc.text\r\n",
        "      if 'part' in url:\r\n",
        "        site_maps.append(url)\r\n",
        "\r\n",
        "    return site_maps"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYFcJfI_es9n",
        "outputId": "102bab11-b335-4857-ee6d-b1d5f837a860"
      },
      "source": [
        "site_map = get_url(link)\r\n",
        "print(\"Foram encontradas {} páginas onde as imagens estão distribuídas.\" .format(len(site_map)))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Foram encontradas 14 páginas onde as imagens estão distribuídas\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzOYoY-YrqvG"
      },
      "source": [
        "Com a função `get_map` é possível obter todas as subpáginas onde as imagens são apresentadas.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQC9afe5sLMr"
      },
      "source": [
        "## Obtendo acesso as imagens\r\n",
        "\r\n",
        "Agora que temos acesso as subpáginas a iremos elaborar uma função para obter o link para todas as imagens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHvGXhVlevON"
      },
      "source": [
        "def master_list(url):\r\n",
        "  \"\"\"\r\n",
        "  essa função tem como parametros:\r\n",
        "    entrada:  lista de subpáginas onde as imagens estão listas obtida na função site_map\r\n",
        "    saída: lista de url das imagens que estão contidas nas subpáginas\r\n",
        "  \"\"\"  \r\n",
        "    images_list = []\r\n",
        "    for loc in soup.find_all('loc'):\r\n",
        "      img_url = loc.text\r\n",
        "      images_list.append(img_url)\r\n",
        "\r\n",
        "    return images_list"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vlHEfxbisxW"
      },
      "source": [
        "# exemplo para não sobrecarregarmos o site\r\n",
        "site_map_1 = site_map[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gRPWNfCyfZWd",
        "outputId": "5173bc4c-4093-48cb-82ac-2d17bc9174fb"
      },
      "source": [
        "# atribuindo a lista de imagens (nesse caso apenas um exemplo) o resultado obtido na  função site_map\r\n",
        "\r\n",
        "img_list = master_list(site_map_1) #estou utilizando a página 1 como exemplo\r\n",
        "print(\"Foram encontradas {} imagens por página.\" .format(len(img_list)))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Foram encontradas 5000 imagens por página.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSAaB6tmsRts"
      },
      "source": [
        "Criamos a função `master_list` que recebe como entrada a lista de subpáginas da função `get_map`, a partir dessa lista a função irá percorrer todas as subpágians e listar todas as urls dda imagens disponíveis.\r\n",
        "\r\n",
        "Nesse ponto criei a variável site_map_1 apenas para esse estudo, já que a não há necessidade de sobrecarregar o servidor nesse estudo.\r\n",
        "\r\n",
        "Veja que para cada subpágina são listadas 5.000 imagens, no momento eu que eu executei esse notebook eram 14 subpáginas, portanto são aproximadamente 70.000 imagens disponíveis.\r\n",
        "\r\n",
        "Para utilizar a função basta atribuir a uma variável a função passando como parâmetro a lista obtida no site_map."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgb6Oiu_tI6G"
      },
      "source": [
        "## Obtendo as Imagens\r\n",
        "\r\n",
        "Agora que estamos com a lista das imagens disponível iremos desenvolver uma função que obtenha as imagens, faça o donwload e renomeie o arquivo mantendo os dados originais do arquivo para posterior utilização."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "igg7tXOWfow4"
      },
      "source": [
        "def get_img(img_list, start=0, end=20):\r\n",
        "  \"\"\"\r\n",
        "  essa função executa o download das imagens\r\n",
        "  parametros \r\n",
        "    entrada:\r\n",
        "      lista de imagens da função master_list\r\n",
        "      para não saturar o servidor foi incluído um início e fim\r\n",
        "        confira o len(img_list) para verificar quantas imagens há por lista\r\n",
        "    saída:\r\n",
        "      salva as imagens no diretorio onde o script está sendo executado\r\n",
        "      as imagens serão salvas mantendo o nome original e o respectivo ID\r\n",
        "  \"\"\"\r\n",
        "  for image in img_list[start:end]:\r\n",
        "    response = requests.get(image)\r\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\r\n",
        "    png_link = soup.find('a', {'class': 'download'})['href']\r\n",
        "    # obtendo o arquivo\r\n",
        "    document = requests.get(png_link)\r\n",
        "    # preservando o ID e nome do arquivo\r\n",
        "    document_title = image.split('/')[-1] + '-' + png_link.split('/')[-1]\r\n",
        "\r\n",
        "    # salvando os arquivos no diretório\r\n",
        "    with open(document_title, 'wb') as file:\r\n",
        "      file.write(document.content)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EaBSQ7m3i5gA"
      },
      "source": [
        "# utilizando a função get_img para realizar o donwload das imagens 1 a 9\r\n",
        "get_img(img_list, 0, 10)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utmdeGzOu1It"
      },
      "source": [
        "Com poucas linhas podemos realizar um scraper do site e obter todas as imagens listas.\r\n",
        "\r\n",
        "Os próximos passos seria implementá-lo em um algorítmo Python para rodá-lo diretamente do computador. Pode-se ainda em pensar nos parâmetros de início e fim diretamente na função rum doo script."
      ]
    }
  ]
}